{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4 nltk gensim\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn \n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉HTML标签的数据\n",
    "#example = BeautifulSoup(df['review'][1000], 'html.parser').get_text()\n",
    "class EnPreprocess:  \n",
    "    def FileRead(self, filePath):#读取内容 \n",
    "        with open(filePath,encoding='UTF-8',errors='ignore') as f:  \n",
    "        #f = open(filePath)  \n",
    "            raw = f.read()\n",
    "            f.close()\n",
    "\n",
    "        return raw  \n",
    "    \n",
    "    def WriteResult(self, result, resultPath): \n",
    "        Path = str(resultPath).split('/')[0] + '/' + 'transform' + '/'\n",
    "        print(\"writepath::::\", Path+'PMP books.txt')\n",
    "        #Path = str(resultPath).replace(str(resultPath).split('/')[-1],'') + 'transform' + '/'\n",
    "        self.mkdir(Path)\n",
    "        #self.mkdir(str(resultPath).replace(str(resultPath).split('/')[-1],''))\n",
    "        f = open(Path+'PMP books.txt','a+',encoding='utf8') #将结果保存到另一个文档中  \n",
    "        for i in result:\n",
    "            if i != '\\n':\n",
    "                f.write(str(i)+' ')\n",
    "            else:\n",
    "                f.write(str(i))\n",
    "        #f.write(str(result)) \n",
    "        f.close()\n",
    "        \n",
    "    def mkdir(self, path):  \n",
    "        # 去除首位空格  \n",
    "        path = path.strip()  \n",
    "        # 去除尾部 \\ 符号  \n",
    "        path = path.rstrip(\"\\\\\")  \n",
    "        # 判断路径是否存在  \n",
    "        # 存在    True  \n",
    "        # 不存在  False  \n",
    "        isExists = os.path.exists(path)\n",
    "        \n",
    "        # 判断结果  \n",
    "        if not isExists:  \n",
    "            # 如果不存在则创建目录  \n",
    "            print(path+'  create successfully!')\n",
    "            print('\\n')\n",
    "            # 创建目录操作函数  \n",
    "            os.makedirs(path)  \n",
    "            return True  \n",
    "        else:  \n",
    "            # 如果目录存在则不创建，并提示目录已存在  \n",
    "            #print(path+'  The directory exists!!')\n",
    "            print('\\n')\n",
    "            return False \n",
    "        \n",
    "    def getHtml(url):\n",
    "        page = urllib.request.urlopen(url)  \n",
    "        html = page.read().decode(\"utf-8\")\n",
    "        soup=bs4.BeautifulSoup(html,'html.parser')\n",
    "        soup.prettify()\n",
    "        tds = soup.find_all('td')\n",
    "\n",
    "        td_content=[]\n",
    "        for i in tds:\n",
    "            #print(i.text) #这里取标签span的内容\n",
    "            td_content.append(i.text)\n",
    "        #print(td_content) \n",
    "        return html\n",
    "\n",
    "        \n",
    "    def SenToken(self, raw):#分割成句子  \n",
    "        sent_tokenizer = nltk.data.load('CopusPreprocess/tokenizers/punkt/english.pickle')  \n",
    "        sents = sent_tokenizer.tokenize(raw)\n",
    "        return sents\n",
    "    \n",
    "    def POSTagger(self, sent):  \n",
    "        taggedLine = [nltk.pos_tag(sent) for sent in sents]  \n",
    "        return taggedLine\n",
    "    \n",
    "    def WordTokener(self, sent):#将单句字符串分割成词  \n",
    "        result=''  \n",
    "        wordsInStr = nltk.word_tokenize(sent)\n",
    "        return wordsInStr\n",
    "    \n",
    "    def WordCheck(self, words):#拼写检查  \n",
    "        d = enchant.Dict(\"en_US\")  \n",
    "        checkedWords = ()  \n",
    "        for word in words:  \n",
    "            if notd.check(word):  \n",
    "                d.suggest(word)  \n",
    "                word = raw_input()  \n",
    "            checkedWords = (checkedWords,'05')  \n",
    "        return checkedWords\n",
    "    \n",
    "    def CleanLines(self, line):  #去除数字和ACSCII符号\n",
    "        a = '[’!\"#$%&\\'•●®()*+,-./:;<=>?@[\\\\]^_`{|}~]+'    \n",
    "        line = re.sub(a, \" \", line)\n",
    "        line1 = re.sub('\\d',\" \",line) #数字\n",
    "        line2 = re.sub('\\n',\" \",line1) #去掉换行符\n",
    "        cleanLine = re.sub('[^a-zA-Z]',' ',line2)\n",
    "        \n",
    "        return cleanLine  \n",
    "    \n",
    "    def CleanWords(self, wordsInStr):#小写化，去掉停用词  \n",
    "        cleanWords = []  \n",
    "          \n",
    "        for words in wordsInStr:  \n",
    "            cleanWords += [[w.lower() for w in words if w.lower() not in stopwords.words('english') and 3<=len(w)]]\n",
    "         \n",
    "            #cleanWords += [w.lower() for w in words if w.lower() not in stopwords.words('english')]\n",
    "            #cleanWords += [w.lower() for w in words if len(w)>=3]\n",
    "            \n",
    "        return cleanWords  \n",
    "    \n",
    "    def StemWords(self, cleanWordsList):  #提取词干\n",
    "        stemWords = []  \n",
    "        #porter = nltk.PorterStemmer() \n",
    "        #stemWords = [porter.stem(t) for t in cleanWordsList]  \n",
    "        \n",
    "        #for words in cleanWordsList:  \n",
    "            #stemWords += [[wn.morphy(w) for w in words]]  \n",
    "            \n",
    "        s = nltk.stem.SnowballStemmer('english')  \n",
    "        stemWords = [s.stem(t) for t in cleanWordsList]  \n",
    "        return stemWords\n",
    "    \n",
    "    def LemmatizeWords(self, cleanWordsList): #词性还原\n",
    "        lemmatizeWords = []\n",
    "        l = WordNetLemmatizer()\n",
    "        lemmatizeWords = [l.lemmatize(t) for t in cleanWordsList]\n",
    "        return lemmatizeWords\n",
    "        \n",
    "    def WordsToStr(self, stemWords):  #转换成字符串\n",
    "        num = 0\n",
    "        strLine = []\n",
    "        for words in stemWords:\n",
    "            for w in words:\n",
    "                num += 1\n",
    "                if num > 20:\n",
    "                    strLine.append('\\n')\n",
    "                    num = 0\n",
    "                else:\n",
    "                    strLine.append(w)\n",
    "            #strLine += [w for w in words]\n",
    "        \n",
    "        \n",
    "        return strLine  \n",
    "        \n",
    "    def EnPreMain(self, dir):\n",
    "        for root,dirs,files in os.walk(dir):\n",
    "            for eachfiles in files:  \n",
    "                dataPath = root + '/' + eachfiles\n",
    "                print(\"CopusPath::::\", dataPath)\n",
    "                raw = self.FileRead(dataPath).strip()\n",
    "                sents = self.SenToken(raw) #分句\n",
    "    #             taggedLine=self.POSTagger(sents)#暂不启用词性标注\n",
    "                cleanLines = [self.CleanLines(line) for line in sents] #清洗句子，去掉标点符号，数字\n",
    "                #print(cleanLines)\n",
    "                words = [self.WordTokener(cl) for cl in cleanLines]#分词\n",
    "    #            checkedWords=self.WordCheck(words)#暂不启用拼写检查\n",
    "                cleanWords = self.CleanWords(words) #转化为小写并且去掉停用词\n",
    "                #stemWords = self.StemWords(cleanWords)#提取词干\n",
    "                #lemmatizeWords = self.LemmatizeWords(cleanWords)#词性还原\n",
    "                #strLine = self.WordsToStr(stemWords)#重新合成为句子\n",
    "                strLine = self.WordsToStr(cleanWords)#重新合成为句子\n",
    "                self.WriteResult(strLine,dataPath)  \n",
    "  \n",
    "        \n",
    "    def StandardTokener(self, raw):  \n",
    "        result=''   \n",
    "        return result \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CopusPath:::: CopusPreprocess/data/PMP books.txt\n",
      "writepath:::: CopusPreprocess/transform/PMP books.txt\n",
      "\n",
      "\n",
      "CopusPath:::: CopusPreprocess/data/PMP Project Management Professional Study Guide1.txt\n",
      "writepath:::: CopusPreprocess/transform/PMP books.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enPre=EnPreprocess()  \n",
    "enPre.EnPreMain('CopusPreprocess/data')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
